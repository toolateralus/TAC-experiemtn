#import core;
#import interned_string;

TType :: enum {
  Invalid,
  Identifier,

  // Literals
  Integer,
  String,

  // Operators.
  Range,

  LParen,
  RParen,
  LCurly,
  RCurly,
  LBrace,
  RBrace,

  Colon,
  DoubleColon,
  Semi,

  Add,
  Sub,
  Mul,
  Div,
  Eof = -1,
}

ttype_name_cache: string[TType.Div];

get_ttype_names :: () {
  type := #type TType;
  for i in 0..TType.Div {
    ttype_name_cache[i] = type.fields[i].name;
  }
}

Token :: struct {
  value:        InternedString;
  line:         s64;
  column:       s64;
  type:         int = -1;
  
  to_string :: () -> string {
    return $"value: --{value.get_string_value()}--, line: {line}, column: {column}, (type: {ttype_name_cache[type]} :: {type})";
  }
}

token_eof :: () -> Token {
  tok: Token;
  tok.type = .Eof;
  return tok;
}

make_token :: (slice: string, line: s64, column, type: int) -> Token {
  token: Token;
  token.value = #make(InternedString, slice);
  token.line = line;
  token.column = column;
  token.type = type;
  return token;
}

make_operator :: (slice: string, line: s64, column) -> Token {
  token: Token;
  token.value = #make(InternedString, slice);
  token.line = line;
  token.column = column;

  token.type = switch slice {
    string{"::"}: { return .DoubleColon; }
    string{".."}: { return .Range; }
    string{"+"}:  { return .Add; }
    string{"-"}:  { return .Sub; }
    string{"/"}:  { return .Div; }
    string{"*"}:  { return .Mul; }
    string{":"}:  { return .Colon; }
    string{"("}:  { return .LParen; }
    string{")"}:  { return .RParen; }
    string{"["}:  { return .LBrace; }
    string{"]"}:  { return .RBrace; }
    string{"{"}:  { return .LCurly; }
    string{"}"}:  { return .RCurly; }
    string{";"}:  { return .Semi; }
  }
  return token;
}

Lexer :: struct {
  #ctor :: (input: string) {
    this.input = input;
    length = input.length;
  }

  position:     int;
  line:         s64       = 1;
  column:       s64       = 1;
  input:        string;
  length:       s64;

  get_token :: () -> Token {
    if position >= length
      then return token_eof();

    while position < length {
      c : char = input[position];
      if c == #char "\n" {
        column = 1;
        position++;
        line++;
        continue;
      }
      if c == #char " " {
        position++;
        column++;
        continue;
      }
      start := position;

      get_slice :: () -> string {
        return #make(string, &input[start], &input[position]);
      }

      advance :: () {
        position++;
        c = input[position];
        column++;
      }

      backtrack :: () {
        position--;
        c = input[position];
        column--;
      }

      get_longest_matching_operator :: () -> Token {
        last_match: Token;
        while position < length && ispunct(c) {
          advance();
          slice := get_slice();
          match := make_operator(slice, line, column);
          if match.type != .Invalid {
            last_match = match;
          } else {
            backtrack();
            break;
          }
        }

        if last_match.type == .Invalid {
          println($"Failed to lex operator {get_slice()}");
          exit(1);
        }
        return last_match;
      }

      if isalpha(c) {
        while position < length && isalpha(c) || c == #char "_" {
          advance();
        }
        slice := get_slice();
        return make_token(slice, line, column, .Identifier);
      } else if isdigit(c) {
        while position < length && isdigit(c) || c == #char "_" {
          advance();
        }
        slice := get_slice();
        return make_token(slice, line, column, .Integer);
      } else if ispunct(c) {
        return get_longest_matching_operator();
      } else {
        printf("Failed to lex character: %c\n", c);
      }
    }

    return token_eof();
  }
}

// main :: () {
//   get_ttype_names();
//   lexer := #make(Lexer, "fn xaryu(int n){ return 0; }");
//   i: int;
//   while true {
//     tok := lexer.get_token();

//     if tok.type == .Eof
//       then break;

//     println(tok.to_string());
//     i++;
//   }
//   println($"lexed {i} tokens");
// }

